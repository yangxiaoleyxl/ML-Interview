### Logistic Regression
#### LR 的假设 
- 假设数据服从伯努利分布 
- 假设样本为正的概率是：
$$
h_{\theta}(x, \theta) = \frac{1}{1+e^{-\theta x}}
$$

#### LR的损失函数 
- LR的损失函数是**极大似然函数**

#### LR的求解方法
- 梯度下降无法直接求解，采用梯度下降法
- 常见梯度下降法：SGD, BGD, Mini-BGD 
- 上述方法还有两个缺点
    - 如何对模型选择合适的 learning rate：动量思想 
    - 如何对参数选择合适的 learning rate：Adam优化器

#### LR的目的 
- 实现二分类
- 将回归的输出与阈值比较，大于阈值判为正，否则为负

#### LR为什么使用极大似然函数作为损失函数
- 极大似然函数取对数后为对数损失函数，LR模型下，对数损失函数的参数求解更新较快

#### 训练过程中，若有很多高度相关的特征，或一个特征重复了100遍，会造成怎样的影响
- 在损失函数函数收敛的情况下，不会影响分类效果
