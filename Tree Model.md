## Decision Tree
### 决策树
决策树是一种基本的分类与回归方法。这里主要讨论决策树用于分类。

决策树模型是描述对样本进行分类的树形结构。树由结点和有向边组成：

- 内部结点表示一个特征或者属性。
- 叶子结点表示一个分类。
- 有向边代表了一个划分规则。
- 决策树从根结点到子结点的的有向边代表了一条路径。决策树的路径是互斥并且是完备的。

用决策树分类时，对样本的某个特征进行测试，根据测试结果将样本分配到树的子结点上。此时每个子结点对应该特征的一个取值。

递归地对样本测试，直到该样本被划分叶结点。最后将样本分配为叶结点所属的类。

决策树的优点：
- 可读性强，
- 分类速度快。

决策树学习通常包括3个步骤：
- 特征选择。
- 决策树生成。
- 决策树剪枝。  

### 梯度提升树 
- Boosting Tree 是以决策树为基本学习器的加法模型 
- 对于分类问题, 使用二叉分类树; 对于回归问题, 使用二叉回归树
- Boosting Tree 采用前向分步算法
    - 初始化提升树, $f_0(x)=0$ 
    - 第 m 步的模型: $f_m{x} = f_{m-1}(x) + h_m(x, \theta)$ 
    - 经验风险极小化确定第m个决策树参数, $\Theta_m = argmin_{\Theta_m} \sum_{i=1}^{N} L(y_i, f_m(x)) $, 此处为引入正则化, XGBoost中引入 
- 回归问题通常使用**平方误差损失函数**, 分类问题通常使用**指数损失函数**
